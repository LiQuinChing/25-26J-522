{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5UG0XsLemom2lBUyEDldr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LiQuinChing/25-26J-522/blob/Ischemia_CAD_detect-Thisal/CAD_Ischemia_D2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 01: Environmental Setup**"
      ],
      "metadata": {
        "id": "ZZSt5FTcPH26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wfdb numpy scipy torch scikit-learn matplotlib pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ildRri75GpE_",
        "outputId": "845c4d39-6895-4ff6-a432-d6060ed2dd3f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wfdb in /usr/local/lib/python3.12/dist-packages (4.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.3.3)\n",
            "Requirement already satisfied: aiohttp>=3.10.11 in /usr/local/lib/python3.12/dist-packages (from wfdb) (3.13.2)\n",
            "Requirement already satisfied: fsspec>=2023.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2025.3.0)\n",
            "Requirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2.32.4)\n",
            "Requirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (0.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (2025.11.12)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.10.0->wfdb) (2.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb) (2.23)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 02: Imports**"
      ],
      "metadata": {
        "id": "GDss3D17PWQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import wfdb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from scipy.signal import butter, filtfilt, resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n"
      ],
      "metadata": {
        "id": "juuwnypfG3Hm"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 03: Mount Google Drive**"
      ],
      "metadata": {
        "id": "8AHx5R4GPbMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i27aVW_kG43O",
        "outputId": "209b1c6c-d3c9-4055-8b01-1fe7d8838888"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 04: Paths**"
      ],
      "metadata": {
        "id": "8G4IFAfOVCxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PTBXL_PATH = \"/content/drive/MyDrive/ECG_Datasets/PTBXL\"\n",
        "EDB_PATH   = \"/content/drive/MyDrive/ECG_Datasets/European-STT\"\n",
        "\n",
        "SAVE_PATH = \"/content/drive/MyDrive/ECG_Datasets\"\n",
        "PTB_X = os.path.join(SAVE_PATH, \"X_ptb.npy\")\n",
        "PTB_Y = os.path.join(SAVE_PATH, \"y_ptb.npy\")\n",
        "EDB_X = os.path.join(SAVE_PATH, \"X_edb.npy\")\n",
        "EDB_Y = os.path.join(SAVE_PATH, \"y_edb.npy\")\n",
        "\n",
        "PTB_CKPT = os.path.join(SAVE_PATH, \"ptbxl_checkpoint.pt\")\n",
        "EDB_CKPT = os.path.join(SAVE_PATH, \"edb_checkpoint.pt\")\n"
      ],
      "metadata": {
        "id": "H3vpgB_sKfS0"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 05: Signal Preprocessing**"
      ],
      "metadata": {
        "id": "HsAfIwpRPliE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bandpass_filter(signal, fs=250):\n",
        "    b, a = butter(4, [0.5/(fs/2), 40/(fs/2)], btype='band')\n",
        "    return filtfilt(b, a, signal, axis=0)\n",
        "\n",
        "def normalize(signal):\n",
        "    return (signal - np.mean(signal)) / (np.std(signal) + 1e-8)\n",
        "\n",
        "def resample_signal(signal, orig_fs, target_fs=250):\n",
        "    n_samples = int(len(signal) * target_fs / orig_fs)\n",
        "    return resample(signal, n_samples)\n"
      ],
      "metadata": {
        "id": "gR6gpsNiKpQ1"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 06: ECG Segmentation**"
      ],
      "metadata": {
        "id": "GkTJZIirPrb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def segment_signal(signal, window=1250, step=1250):\n",
        "    return np.array([\n",
        "        signal[i:i+window]\n",
        "        for i in range(0, len(signal)-window, step)\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "hUehqjPHKsOs"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 07: PTB-XL Loader (PRETRAINING)**"
      ],
      "metadata": {
        "id": "hYVEexjtPw8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load metadata"
      ],
      "metadata": {
        "id": "PmcTNKuCP4vN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "meta = pd.read_csv(os.path.join(PTBXL_PATH, \"ptbxl_database.csv\"))\n",
        "meta = meta.sample(3000, random_state=42)\n"
      ],
      "metadata": {
        "id": "j4psJ4L3KuWb"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label mapping (ischemia-related)"
      ],
      "metadata": {
        "id": "r4V9Wn8IP7Od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_ischemia(row):\n",
        "    scp = eval(row[\"scp_codes\"])\n",
        "    return int(any(k in scp for k in [\"ISC_\", \"MI\"]))\n"
      ],
      "metadata": {
        "id": "suXdA66VK0a1"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load PTB-XL ECG"
      ],
      "metadata": {
        "id": "hKvCsv0KP--V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_ptbxl_record(record_path):\n",
        "    record = wfdb.rdrecord(record_path)\n",
        "    signal = record.p_signal[:, [1, 6]]  # Lead II, V5\n",
        "    signal = resample_signal(signal, record.fs)\n",
        "    signal = bandpass_filter(signal)\n",
        "    signal = normalize(signal)\n",
        "    return segment_signal(signal)\n",
        "\n"
      ],
      "metadata": {
        "id": "Atyt8nKmK1yD"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 08: Build or Load Preprocessed PTB-XL**"
      ],
      "metadata": {
        "id": "ERGPxuFPQB9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(PTB_X) and os.path.exists(PTB_Y):\n",
        "    print(\"Loading saved PTB-XL data...\")\n",
        "    X_ptb = np.load(PTB_X)\n",
        "    y_ptb = np.load(PTB_Y)\n",
        "\n",
        "else:\n",
        "    print(\"Processing PTB-XL from scratch...\")\n",
        "    X_ptb, y_ptb = [], []\n",
        "\n",
        "    for _, row in meta.iterrows():\n",
        "        try:\n",
        "            record_path = os.path.join(PTBXL_PATH, row[\"filename_hr\"])\n",
        "            segments = load_ptbxl_record(record_path)\n",
        "\n",
        "            label = int(any(\n",
        "                k.startswith((\"ISC\", \"MI\"))\n",
        "                for k in eval(row[\"scp_codes\"]).keys()\n",
        "            ))\n",
        "\n",
        "            X_ptb.extend(segments)\n",
        "            y_ptb.extend([label] * len(segments))\n",
        "\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    X_ptb = np.array(X_ptb)\n",
        "    y_ptb = np.array(y_ptb)\n",
        "\n",
        "    np.save(PTB_X, X_ptb)\n",
        "    np.save(PTB_Y, y_ptb)\n",
        "\n",
        "print(\"PTB-XL samples:\", len(X_ptb))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsGatTdlK5J3",
        "outputId": "47d8ed43-bb09-48a2-b06c-68dd08e075ec"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing PTB-XL from scratch...\n",
            "PTB-XL samples: 3000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"PTB-XL segments:\", len(X_ptb))\n",
        "print(\"PTB-XL labels:\", len(y_ptb))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_riwmtIM64f",
        "outputId": "1fdcda4e-e258-4792-878b-55de55aafca5"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PTB-XL segments: 3000\n",
            "PTB-XL labels: 3000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 09: PyTorch Dataset**"
      ],
      "metadata": {
        "id": "5sCM20zoQPDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ECGDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n"
      ],
      "metadata": {
        "id": "0PmQQU28LHU6"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 10: CNN-LSTM Model**"
      ],
      "metadata": {
        "id": "YSfdKPDaQUxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_LSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # self.cnn = nn.Sequential(\n",
        "        #     nn.Conv1d(2, 32, 7, padding=3),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.MaxPool1d(2),\n",
        "        #     nn.Conv1d(32, 64, 5, padding=2),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.MaxPool1d(2)\n",
        "        # )\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(2, 32, 7, padding=3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Conv1d(32, 64, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "\n",
        "        self.lstm = nn.LSTM(64, 64, batch_first=True)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # self.fc = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.cnn(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        _, (hn, _) = self.lstm(x)\n",
        "        return self.fc(hn[-1]).squeeze()\n"
      ],
      "metadata": {
        "id": "3RjYmusBLKBS"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 11: Pretraining on PTB-XL**"
      ],
      "metadata": {
        "id": "K6ED6i8IQaRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "    X_ptb, y_ptb, test_size=0.2, stratify=y_ptb, random_state=42\n",
        ")\n",
        "\n",
        "# train_ds = ECGDataset(X_ptb, y_ptb)\n",
        "train_ds = ECGDataset(X_tr, y_tr)\n",
        "val_ds   = ECGDataset(X_val, y_val)\n",
        "\n",
        "# train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "# val_loader   = DataLoader(val_ds, batch_size=64, shuffle=False)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
        "\n",
        "model = CNN_LSTM()\n",
        "# criterion = nn.BCELoss()\n",
        "# criterion = nn.BCEWithLogitsLoss()\n",
        "pos_weight = torch.tensor(\n",
        "    (len(y_ptb) - np.sum(y_ptb)) / np.sum(y_ptb),\n",
        "    dtype=torch.float32\n",
        ")\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "start_epoch = 0\n",
        "if os.path.exists(PTB_CKPT):\n",
        "    ckpt = torch.load(PTB_CKPT)\n",
        "    model.load_state_dict(ckpt[\"model\"])\n",
        "    optimizer.load_state_dict(ckpt[\"opt\"])\n",
        "    start_epoch = ckpt[\"epoch\"] + 1\n",
        "    print(\"Resuming PTB-XL training from epoch\", start_epoch)\n",
        "\n",
        "for epoch in range(start_epoch, 5):\n",
        "    model.train()\n",
        "    for x, y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(x), y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    torch.save({\n",
        "        \"epoch\": epoch,\n",
        "        \"model\": model.state_dict(),\n",
        "        \"opt\": optimizer.state_dict()\n",
        "    }, PTB_CKPT)\n",
        "\n",
        "    print(f\"PTB-XL Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    val_logits, val_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            val_logits.append(model(x))\n",
        "            val_labels.append(y)\n",
        "\n",
        "    val_logits = torch.cat(val_logits)\n",
        "    val_labels = torch.cat(val_labels)\n",
        "    val_auc = roc_auc_score(\n",
        "        val_labels.cpu().numpy(),\n",
        "        torch.sigmoid(val_logits).cpu().numpy()\n",
        "    )\n",
        "\n",
        "    print(\"Validation AUC:\", round(val_auc, 3))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dwox2PmOLMkD",
        "outputId": "09d6cd82-7256-47cc-eb99-a0f6af10772a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PTB-XL Epoch 1, Loss: 1.4557\n",
            "Validation AUC: 0.648\n",
            "PTB-XL Epoch 2, Loss: 1.4415\n",
            "Validation AUC: 0.684\n",
            "PTB-XL Epoch 3, Loss: 1.2879\n",
            "Validation AUC: 0.699\n",
            "PTB-XL Epoch 4, Loss: 1.5478\n",
            "Validation AUC: 0.682\n",
            "PTB-XL Epoch 5, Loss: 1.4474\n",
            "Validation AUC: 0.697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 12: Freeze CNN (Transfer Learning)**"
      ],
      "metadata": {
        "id": "ppmXphKCQqot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for p in model.cnn.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# for name, p in model.cnn.named_parameters():\n",
        "#     if \"0\" in name:  # first conv only\n",
        "#         p.requires_grad = False\n",
        "\n"
      ],
      "metadata": {
        "id": "bEybhj0jQxBz"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 13: Load European ST-T**"
      ],
      "metadata": {
        "id": "yANWVJlNWNth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_edb_record(name):\n",
        "    rec = wfdb.rdrecord(os.path.join(EDB_PATH, name))\n",
        "    ann = wfdb.rdann(os.path.join(EDB_PATH, name), 'atr')\n",
        "\n",
        "    signal = normalize(bandpass_filter(rec.p_signal))\n",
        "    segments = segment_signal(signal)\n",
        "    labels = np.zeros(len(segments))\n",
        "\n",
        "    for s in ann.sample:\n",
        "        idx = s // 1250\n",
        "        if idx < len(labels):\n",
        "            labels[idx] = 1\n",
        "\n",
        "    return segments, labels\n",
        "\n",
        "if os.path.exists(EDB_X) and os.path.exists(EDB_Y):\n",
        "    print(\"Loading cached European ST-T...\")\n",
        "    X_edb = np.load(EDB_X)\n",
        "    y_edb = np.load(EDB_Y)\n",
        "else:\n",
        "    X_edb, y_edb = [], []\n",
        "    files = [f for f in os.listdir(EDB_PATH) if f.endswith(\".hea\")]\n",
        "    for f in files:\n",
        "        s, l = load_edb_record(f.replace(\".hea\", \"\"))\n",
        "        X_edb.extend(s)\n",
        "        y_edb.extend(l)\n",
        "    X_edb = np.array(X_edb)\n",
        "    y_edb = np.array(y_edb)\n",
        "    np.save(EDB_X, X_edb)\n",
        "    np.save(EDB_Y, y_edb)\n"
      ],
      "metadata": {
        "id": "zAQYcQo2WRnw"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 14: Fine-Tune on European ST-T**"
      ],
      "metadata": {
        "id": "Ry7lvLeMQy9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training and validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_edb, y_edb, test_size=0.2, stratify=y_edb, random_state=42\n",
        ")\n",
        "\n",
        "edb_ds = ECGDataset(X_train, y_train)\n",
        "val_ds   = ECGDataset(X_val, y_val)\n",
        "\n",
        "edb_loader = DataLoader(edb_ds, batch_size=32, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
        "\n",
        "# edb_ds = ECGDataset((X_edb), (y_edb))\n",
        "# edb_loader = DataLoader(edb_ds, batch_size=32, shuffle=True)\n",
        "\n",
        "# Freeze the entire CNN backbone\n",
        "for p in model.cnn.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Only fine-tune LSTM + FC layers\n",
        "optimizer = torch.optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=1e-5\n",
        ")\n",
        "\n",
        "# optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
        "\n",
        "# BCEWithLogitsLoss with pos_weight for imbalance\n",
        "pos_weight = torch.tensor((len(y_train) - np.sum(y_train)) / np.sum(y_train), dtype=torch.float32)\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "start_epoch = 0\n",
        "if os.path.exists(EDB_CKPT):\n",
        "    ckpt = torch.load(EDB_CKPT)\n",
        "    model.load_state_dict(ckpt[\"model\"])\n",
        "    optimizer.load_state_dict(ckpt[\"opt\"])\n",
        "    start_epoch = ckpt[\"epoch\"] + 1\n",
        "    print(\"Resuming EDB training from epoch\", start_epoch)\n",
        "\n",
        "# for epoch in range(start_epoch, 5):\n",
        "#     model.train()\n",
        "#     for x, y in edb_loader:\n",
        "#         optimizer.zero_grad()\n",
        "#         loss = criterion(model(x), y)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#     torch.save({\n",
        "#         \"epoch\": epoch,\n",
        "#         \"model\": model.state_dict(),\n",
        "#         \"opt\": optimizer.state_dict()\n",
        "#     }, EDB_CKPT)\n",
        "\n",
        "#     print(f\"EDB Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Fine-tuning with validation\n",
        "best_val_loss = float('inf')\n",
        "for epoch in range(start_epoch, 5):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(x), y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * len(y)\n",
        "\n",
        "    train_loss /= len(train_ds)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_preds, val_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            val_loss += loss.item() * len(y)\n",
        "            val_preds.append(torch.sigmoid(logits))\n",
        "            val_labels.append(y)\n",
        "    val_loss /= len(val_ds)\n",
        "    val_preds = torch.cat(val_preds)\n",
        "    val_labels = torch.cat(val_labels)\n",
        "    val_auc = roc_auc_score(val_labels.cpu().numpy(), val_preds.cpu().numpy())\n",
        "\n",
        "    print(f\"EDB Epoch {epoch+1} → Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val AUC: {val_auc:.3f}\")\n",
        "\n",
        "    # Save checkpoint if validation improves\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"model\": model.state_dict(),\n",
        "            \"opt\": optimizer.state_dict()\n",
        "        }, EDB_CKPT)\n",
        "        print(\"Saved best model checkpoint.\")\n"
      ],
      "metadata": {
        "id": "MiPi23uYQ40k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e20a57d0-1387-4a7e-fe4f-4e58df8a041e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EDB Epoch 1 → Train Loss: 0.6938, Val Loss: 0.0001, Val AUC: 0.001\n",
            "Saved best model checkpoint.\n",
            "EDB Epoch 2 → Train Loss: 0.6819, Val Loss: 0.0001, Val AUC: 0.001\n",
            "EDB Epoch 3 → Train Loss: 0.6723, Val Loss: 0.0001, Val AUC: 0.001\n",
            "EDB Epoch 4 → Train Loss: 0.6646, Val Loss: 0.0001, Val AUC: 0.001\n",
            "EDB Epoch 5 → Train Loss: 0.6584, Val Loss: 0.0001, Val AUC: 0.001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 15: Inference**"
      ],
      "metadata": {
        "id": "ht8huVxsQ7Ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def predict_ecg(signal):\n",
        "#     signal = bandpass_filter(signal)\n",
        "#     signal = normalize(signal)\n",
        "#     segments = segment_signal(signal)\n",
        "\n",
        "#     model.eval()\n",
        "#     preds = []\n",
        "#     with torch.no_grad():\n",
        "#         for seg in segments:\n",
        "#             preds.append(model(torch.tensor(seg).unsqueeze(0).float()).item())\n",
        "#     return np.mean(preds)\n",
        "\n",
        "def predict_ecg(signal):\n",
        "    signal = normalize(bandpass_filter(signal))\n",
        "    segments = segment_signal(signal)\n",
        "\n",
        "    model.eval()\n",
        "    probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for seg in segments:\n",
        "            logit = model(torch.tensor(seg).unsqueeze(0).float())\n",
        "            prob = torch.sigmoid(logit).item()\n",
        "            probs.append(prob)\n",
        "\n",
        "    return float(np.mean(probs))\n",
        "\n"
      ],
      "metadata": {
        "id": "tgP6Vc0VQ9Zs"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 16: Testing CAD**"
      ],
      "metadata": {
        "id": "w1BOJZw0AGGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_record = \"e0611\"\n",
        "record = wfdb.rdrecord(os.path.join(EDB_PATH, test_record))\n",
        "\n",
        "test_signal = record.p_signal[:, :2]  # Use first 2 leads\n",
        "score = predict_ecg(test_signal)\n",
        "\n",
        "print(\"CAD Probability:\", round(score, 3))\n",
        "confidence = score * 100\n",
        "print(f\"Confidence: {confidence:.1f}%\")\n",
        "\n",
        "if score >= 0.5:\n",
        "    print(\"Result: CAD (Ischemia) Detected\")\n",
        "else:\n",
        "    print(\"Result: No CAD Detected\")\n"
      ],
      "metadata": {
        "id": "gkIQQy-fAMDs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddad1fc0-4889-41f3-8a68-fd46315dba54"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CAD Probability: 0.519\n",
            "Confidence: 51.9%\n",
            "Result: CAD (Ischemia) Detected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for f in [\"e0207\", \"e0103\", \"e0110\"]:\n",
        "    record = wfdb.rdrecord(os.path.join(EDB_PATH, f))\n",
        "    score = predict_ecg(record.p_signal[:, :2])  # first 2 leads\n",
        "    print(f\"{f} → CAD Probability: {score:.3f}\")\n",
        "    if score >= 0.5:\n",
        "        print(\"Result: CAD (Ischemia) Detected\")\n",
        "    else:\n",
        "        print(\"Result: No CAD\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3h8DiF1S5EsR",
        "outputId": "f72b07ef-0af5-4156-a3fd-fca6f61f4948"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e0207 → CAD Probability: 0.514\n",
            "Result: CAD (Ischemia) Detected\n",
            "e0103 → CAD Probability: 0.509\n",
            "Result: CAD (Ischemia) Detected\n",
            "e0110 → CAD Probability: 0.512\n",
            "Result: CAD (Ischemia) Detected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for f in [\"e0207\", \"e0103\", \"e0110\"]:\n",
        "    record = wfdb.rdrecord(os.path.join(EDB_PATH, f))\n",
        "    score = predict_ecg(record.p_signal[:, :2])\n",
        "    print(f, \"→\", round(score, 3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYMAR_GiMqb2",
        "outputId": "c3ae1ebf-e745-422d-e638-975e5c251e61"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e0207 → 0.514\n",
            "e0103 → 0.509\n",
            "e0110 → 0.512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluations**"
      ],
      "metadata": {
        "id": "0xr0BMf0bxBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_roc_auc(model, loader):\n",
        "    model.eval()\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            logits = model(x)\n",
        "            probs = torch.sigmoid(logits)  # IMPORTANT\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "            all_labels.extend(y.cpu().numpy())\n",
        "\n",
        "    auc = roc_auc_score(all_labels, all_probs)\n",
        "    return auc"
      ],
      "metadata": {
        "id": "QCYVZc6mZiI6"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auc = evaluate_roc_auc(model, edb_loader)\n",
        "print(\"European ST-T ROC-AUC:\", round(auc, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b4eg_gLZljA",
        "outputId": "85802ba7-d6b6-4ad8-99a2-308e13838a3c"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "European ST-T ROC-AUC: 0.9369\n"
          ]
        }
      ]
    }
  ]
}